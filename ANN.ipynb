{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python notebook for Make Your Own Neural Network\n",
    "# code for a 3-layer neural network, and code for learning the MNIST dataset\n",
    "# (c) Tariq Rashid, 2016\n",
    "# license is GPLv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "# scipy.special for the sigmoid function expit()\n",
    "import scipy.special\n",
    "# library for plotting arrays\n",
    "import matplotlib.pyplot\n",
    "# ensure the plots are inside this notebook, not an external window\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network class definition\n",
    "class neuralNetwork:\n",
    "    \n",
    "    \n",
    "    # initialise the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # set number of nodes in each input, hidden, output layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # link weight matrices, wih and who\n",
    "        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        # w11 w21\n",
    "        # w12 w22 etc \n",
    "        self.wih = numpy.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes, self.hnodes))\n",
    "\n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        # activation function is the sigmoid function\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    \n",
    "    # train the neural network\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        targets = numpy.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # output layer error is the (target - actual)\n",
    "        output_errors = targets - final_outputs\n",
    "        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "        hidden_errors = numpy.dot(self.who.T, output_errors) \n",
    "        \n",
    "        # update the weights for the links between the hidden and output layers\n",
    "        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))\n",
    "        \n",
    "        # update the weights for the links between the input and hidden layers\n",
    "        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))\n",
    "        \n",
    "        pass\n",
    "\n",
    "    \n",
    "    # query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifications to the ANN Code\r\n",
    "\r\n",
    "### Overview\r\n",
    "The provided code outlines the process of training and testing an Artificial Neural Network (ANN) on the MNIST dataset. The key modifications introduced focus on enhancing the training flexibility by looping through different dataset files and providing comprehensive performance reports using `sklearn`.\r\n",
    "\r\n",
    "### Key Changes\r\n",
    "\r\n",
    "1. **File Looping for Training Data**:\r\n",
    "    - Added capability to loop through multiple training files, facilitating easy training on the original dataset, a balanced dataset, and several imbalanced datasets.\r\n",
    "    - Imbalanced datasets are generated for different percentages and digits, allowing for a diverse training experience.\r\n",
    "\r\n",
    "2. **Performance Reporting**:\r\n",
    "    - Incorporated `sklearn` for comprehensive performance reporting.\r\n",
    "    - After testing the neural network, the code computes:\r\n",
    "        - A confusion matrix, offering insights into true vs. predicted classifications.\r\n",
    "        - Precision, Recall, and F1-score for each class.\r\n",
    "        - ROC AUC (Receiver Operating Characteristic Area Under the Curve) score for multi-class classification. The macro-average ROC AUC is also computed and displayed.\r\n",
    "\r\n",
    "3. **Data Loading and Pre-processing**:\r\n",
    "    - The training and testing data files are loaded separately within the `train_and_test` function.\r\n",
    "    - The pixel values of the images are normalized to fall between 0.01 and 0.99.\r\n",
    "\r\n",
    "4. **Neural Network Testing**:\r\n",
    "    - During testing, the neural network's outputs for each test instance are collected.\r\n",
    "    - These outputs are further used for computing performance metrics.\r\n",
    "\r\n",
    "### Implementation Details\r\n",
    "- The neural network is instantiated with 784 input nodes (corresponding to the 28x28 pixel MNIST images), 200 hidden nodes, and 10 output nodes (for the 10 digit classes).\r\n",
    "- The learning rate is set at 0.1.\r\n",
    "- The neural network is trained for 5 epochs on each dataset file.\r\n",
    "- The test dataset remains consistent for all training files.\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "These modifications enhance the code's adaptability to different training scenarios and provide a detailed performance breakdown for each training iteration.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 976    0    0    0    0    1    1    1    1    0]\n",
      " [   0 1126    2    1    0    1    2    0    3    0]\n",
      " [  11    3  994    8    0    0    3    7    5    1]\n",
      " [   2    0    2  987    1    2    0    5    5    6]\n",
      " [   3    0    2    0  948    0    5    0    1   23]\n",
      " [   4    1    0   18    0  853    5    1    6    4]\n",
      " [  13    3    0    1    1    8  928    0    3    1]\n",
      " [   4    8    7    1    1    0    0  989    0   18]\n",
      " [   5    1    1    8    5    4    3    3  941    3]\n",
      " [   7    4    1    7    6    3    1    4    3  973]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9522    0.9959    0.9736       980\n",
      "           1     0.9825    0.9921    0.9873      1135\n",
      "           2     0.9851    0.9632    0.9740      1032\n",
      "           3     0.9573    0.9772    0.9672      1010\n",
      "           4     0.9854    0.9654    0.9753       982\n",
      "           5     0.9782    0.9563    0.9671       892\n",
      "           6     0.9789    0.9687    0.9738       958\n",
      "           7     0.9792    0.9621    0.9706      1028\n",
      "           8     0.9721    0.9661    0.9691       974\n",
      "           9     0.9456    0.9643    0.9549      1009\n",
      "\n",
      "    accuracy                         0.9715     10000\n",
      "   macro avg     0.9717    0.9711    0.9713     10000\n",
      "weighted avg     0.9718    0.9715    0.9715     10000\n",
      "\n",
      "AUC-ROC (Macro average): 0.9986\n",
      "Performance for mnist_dataset/mnist_train.csv =  0.9715\n",
      "Confusion Matrix:\n",
      "[[ 974    0    1    1    0    0    0    1    3    0]\n",
      " [   0 1119    3    2    0    2    2    2    5    0]\n",
      " [   2    0 1011    7    1    1    0    8    1    1]\n",
      " [   0    0    3  996    0    4    0    3    3    1]\n",
      " [   1    0    2    0  968    0    3    0    0    8]\n",
      " [   5    0    0   21    2  847    8    2    5    2]\n",
      " [   6    2    2    0    2    2  940    0    4    0]\n",
      " [   3    2    7    3    3    1    0 1003    0    6]\n",
      " [   7    0    2    8    7    2    2    5  938    3]\n",
      " [   4    3    2   16   19    3    1    7    1  953]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9721    0.9939    0.9828       980\n",
      "           1     0.9938    0.9859    0.9898      1135\n",
      "           2     0.9787    0.9797    0.9792      1032\n",
      "           3     0.9450    0.9861    0.9651      1010\n",
      "           4     0.9661    0.9857    0.9758       982\n",
      "           5     0.9826    0.9496    0.9658       892\n",
      "           6     0.9833    0.9812    0.9822       958\n",
      "           7     0.9728    0.9757    0.9743      1028\n",
      "           8     0.9771    0.9630    0.9700       974\n",
      "           9     0.9784    0.9445    0.9612      1009\n",
      "\n",
      "    accuracy                         0.9749     10000\n",
      "   macro avg     0.9750    0.9745    0.9746     10000\n",
      "weighted avg     0.9751    0.9749    0.9749     10000\n",
      "\n",
      "AUC-ROC (Macro average): 0.9982\n",
      "Performance for mnist_dataset/mnist_balanced.csv =  0.9749\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import scipy.special\n",
    "import matplotlib.pyplot\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "# Ensure the plots are inside this notebook, not an external window\n",
    "%matplotlib inline\n",
    "\n",
    "# [Original neuralNetwork class definition here...]\n",
    "\n",
    "def train_and_test(training_file, test_file):\n",
    "    # Load the training data\n",
    "    training_data_file = open(training_file, 'r')\n",
    "    training_data_list = training_data_file.readlines()\n",
    "    training_data_file.close()\n",
    "\n",
    "    # Train the neural network\n",
    "    for e in range(epochs):\n",
    "        for record in training_data_list:\n",
    "            all_values = record.split(',')\n",
    "            inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "            targets = numpy.zeros(output_nodes) + 0.01\n",
    "            targets[int(all_values[0])] = 0.99\n",
    "            n.train(inputs, targets)\n",
    "        pass\n",
    "\n",
    "    # Load the test data\n",
    "    test_data_file = open(test_file, 'r')\n",
    "    test_data_list = test_data_file.readlines()\n",
    "    test_data_file.close()\n",
    "\n",
    "    # Test the neural network and collect scorecard\n",
    "    # Test the neural network and collect scorecard\n",
    "    scorecard = []\n",
    "    all_outputs = []  # Store all network outputs\n",
    "    all_labels = []   # Store all true labels\n",
    "    \n",
    "    for record in test_data_list:\n",
    "        all_values = record.split(',')\n",
    "        correct_label = int(all_values[0])\n",
    "        inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "        outputs = n.query(inputs)\n",
    "        label = numpy.argmax(outputs)\n",
    "        scorecard.append(label)\n",
    "        all_outputs.append(outputs.ravel())\n",
    "        all_labels.append(correct_label)\n",
    "    \n",
    "    all_outputs = numpy.array(all_outputs)\n",
    "    all_labels = numpy.array(all_labels)\n",
    "    \n",
    "    # Computing confusion matrix\n",
    "    cm = confusion_matrix(all_labels, scorecard)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Compute Precision, Recall, and F1-score\n",
    "    report = classification_report(all_labels, scorecard, digits=4)\n",
    "    print(report)\n",
    "    \n",
    "    # Compute ROC AUC\n",
    "    # Binarize the labels and predictions\n",
    "    binarized_labels = label_binarize(all_labels, classes=[0,1,2,3,4,5,6,7,8,9])\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(output_nodes):\n",
    "        roc_auc[i] = roc_auc_score(binarized_labels[:, i], all_outputs[:, i])\n",
    "    # Compute macro-average ROC AUC\n",
    "    auc = numpy.mean(list(roc_auc.values()))\n",
    "    print(f\"AUC-ROC (Macro average): {auc:.4f}\")\n",
    "\n",
    "    scorecard_array = numpy.asarray(scorecard)\n",
    "    performance = numpy.mean(scorecard_array == all_labels)\n",
    "    print(f\"Performance for {training_file} = \", performance)\n",
    "    return performance\n",
    "\n",
    "# Create an instance of the neural network\n",
    "input_nodes = 784\n",
    "hidden_nodes = 200\n",
    "output_nodes = 10\n",
    "learning_rate = 0.1\n",
    "n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "# File paths\n",
    "train_files = [\n",
    "    \"mnist_dataset/mnist_train.csv\",\n",
    "    \"mnist_dataset/mnist_balanced.csv\",\n",
    "]\n",
    "\n",
    "# Add the imbalance files for each percentage and digit\n",
    "for percentage in [2, 5, 8, 15, 20, 50, 80]:\n",
    "    for digit in range(10):\n",
    "        train_files.append(f\"mnist_dataset/mnist_imbalanced_{digit}_{percentage}.csv\")\n",
    "\n",
    "# Assuming test data is same for all\n",
    "test_file = \"mnist_dataset/mnist_test.csv\"\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 5\n",
    "\n",
    "for train_file in train_files:\n",
    "    train_and_test(train_file, test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
